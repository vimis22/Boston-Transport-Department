{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaafd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sysconfig\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718c9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stackable/spark/python/pyspark/sql/connect/conf.py:64: UserWarning: Failed to set spark.sql.catalogImplementation to Some(hive) due to [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.sql.catalogImplementation\".\n",
      "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110\n",
      "  warnings.warn(warn)\n",
      "/stackable/spark/python/pyspark/sql/connect/conf.py:64: UserWarning: Failed to set spark.sql.warehouse.dir to Some(/user/hive/warehouse) due to [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.sql.warehouse.dir\".\n",
      "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .remote(\"sc://spark-connect-server:15002\")\n",
    "        .appName(\"hive\")\n",
    "        .enableHiveSupport()\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Load the Spark Connect jar\n",
    "# spark_connect_jar = f\"{sysconfig.get_paths()['purelib']}/pyspark/jars/spark-connect_2.13-4.0.1.jar\"\n",
    "# spark.addArtifacts(spark_connect_jar)\n",
    "spark.addArtifacts(\"/stackable/spark/connect/spark-connect-4.0.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1b33c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+--------------------+--------------------+----------------------+-----------------------+------------------+--------------------+--------------------+---------------------+---------+\n",
      "|        tripduration|          starttime|           stoptime|    start_station_id|  start_station_name|start_station_latitude|start_station_longitude|    end_station_id|    end_station_name|end_station_latitude|end_station_longitude|partition|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+----------------------+-----------------------+------------------+--------------------+--------------------+---------------------+---------+\n",
      "|-5.753154655473514E7|                 ut|Ut irure qui cillum|occaecat veniam a...|ullamco esse elit...|     -9677097.79029338|    5.822139750931066E7|est in elit dolore|          quis ipsum|-5.564325299423254E7|    8443665.855954796|        0|\n",
      "|              1326.0|2018-01-05T16:05:03|2018-01-05T16:27:09|                   3|Colleges of the F...|     42.34011512249236|     -71.10061883926392|               152|Ink Block - Harri...|           42.345901|           -71.063187|        0|\n",
      "|               430.0|2018-01-05T16:10:14|2018-01-05T16:17:24|                  51|Washington St at ...|      42.3350989929096|     -71.07903778553009|                57|Columbus Ave at M...|  42.340542615516355|   -71.08138847914233|        0|\n",
      "|               471.0|2018-01-05T16:16:45|2018-01-05T16:24:37|                 121|W Broadway at Dor...|             42.335693|             -71.045859|                63|Dorchester Ave at...|  42.344040510016356|   -71.05737626552582|        0|\n",
      "|               614.0|2018-01-05T16:22:40|2018-01-05T16:32:54|                  67|MIT at Mass Ave /...|               42.3581|             -71.093198|                36|Copley Square - D...|   42.34976701008725|   -71.07732653617859|        0|\n",
      "|               786.0|2018-01-05T16:26:48|2018-01-05T16:39:54|                 184|Sidney Research C...|     42.35775309465199|     -71.10393404960632|               189|           Kendall T|  42.362427842912396|   -71.08495473861694|        0|\n",
      "|               571.0|2018-01-05T16:28:50|2018-01-05T16:38:21|                  60|Charles Circle - ...|     42.36075761041193|     -71.07132909824031|                23|Boston City Hall ...|            42.35892|           -71.057629|        0|\n",
      "|               401.0|2018-01-05T16:36:36|2018-01-05T16:43:17|                 141|      Kendall Street|    42.363560158429884|     -71.08216792345047|                84|CambridgeSide Gal...|           42.366981|           -71.076472|        0|\n",
      "|               622.0|2018-01-05T16:38:39|2018-01-05T16:49:01|                  17|Soldiers Field Pa...|     42.36514362978903|     -71.11987173557281|               180|           Mt Auburn|   42.37478628706384|   -71.13320231437683|        0|\n",
      "|               473.0|2018-01-05T16:41:53|2018-01-05T16:49:46|                  70|Harvard Kennedy S...|      42.3722168027866|     -71.12188071012497|               104|Harvard Universit...|           42.380287|           -71.125107|        0|\n",
      "+--------------------+-------------------+-------------------+--------------------+--------------------+----------------------+-----------------------+------------------+--------------------+--------------------+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS weather_data (name VARCHAR(64), age INT, gpa DECIMAL(3, 2)) STORED AS PARQUET LOCATION 'hdfs://hdfs-cluster-namenode-default-0.hdfs-cluster-namenode-default.bigdata.svc.cluster.local/user/hive/warehouse/weater_data'\")\n",
    "#spark.sql(\"INSERT INTO students (name, age, gpa) VALUES ('Hello world123456', 20, 3.5)\")\n",
    "#spark.sql(\"REFRESH TABLE students\")\n",
    "# spark.sql(\"\"\"WITH rides AS (\n",
    "#   SELECT\n",
    "#     concat_ws('|', starttime, start_station_id, end_station_id, tripduration) AS ride_id,\n",
    "#     tripduration,\n",
    "#     starttime\n",
    "#   FROM bike_data\n",
    "# ),\n",
    "# wx AS (\n",
    "#   SELECT\n",
    "#     observation_date,\n",
    "#     dry_bulb_temperature_celsius\n",
    "#   FROM weather_data\n",
    "# ),\n",
    "# matched AS (\n",
    "#   SELECT\n",
    "#     r.ride_id,\n",
    "#     r.tripduration,\n",
    "#     r.starttime,\n",
    "#     w.dry_bulb_temperature_celsius,\n",
    "#     abs(\n",
    "#       unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") -\n",
    "#       unix_timestamp(w.observation_date, \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "#     ) AS diff_seconds_to_obs,\n",
    "#     row_number() OVER (\n",
    "#       PARTITION BY r.ride_id\n",
    "#       ORDER BY abs(\n",
    "#         unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") -\n",
    "#         unix_timestamp(w.observation_date, \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "#       )\n",
    "#     ) AS rn\n",
    "#   FROM rides r\n",
    "#   JOIN wx w\n",
    "#     ON w.observation_date BETWEEN\n",
    "#          from_unixtime(unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") - 3600)\n",
    "#      AND from_unixtime(unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") + 3600)\n",
    "# )\n",
    "# SELECT\n",
    "#   ride_id,\n",
    "#   tripduration,\n",
    "#   starttime,\n",
    "#   dry_bulb_temperature_celsius AS matched_temp_c,\n",
    "#   diff_seconds_to_obs\n",
    "# FROM matched\n",
    "# WHERE rn = 1;\"\"\")\n",
    "#spark.sql(\"clear cache\")\n",
    "#spark.sql(\"INSERT OVERWRITE TABLE students SELECT * FROM students;\")\n",
    "# mydf = spark.sql(\"select * from weather_data\")\n",
    "# mydf.show()\n",
    "\n",
    "mydf = spark.sql(\"select * from bike_data limit 10\")\n",
    "mydf.show()\n",
    "\n",
    "#CREATE TABLE students ( name STRING, age INT, gpa DECIMAL(3,2) ) STORED AS PARQUET TBLPROPERTIES ('transactional'='true') LOCATION 'hdfs://hdfs-cluster-namenode-default-0.hdfs-cluster-namenode-default.bigdata.svc.cluster.local/user/hive/warehouse/students2';\n",
    "#INSERT OVERWRITE TABLE students SELECT * FROM students;\n",
    "#spark.sql(\"ALTER TABLE students3 SET TBLPROPERTIES('transactional'='true')\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
