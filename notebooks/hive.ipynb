{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaafd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sysconfig\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "718c9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mathiasgredal/git/Boston-Transport-Department/.venv/lib/python3.10/site-packages/pyspark/sql/connect/conf.py:64: UserWarning: Failed to set spark.sql.catalogImplementation to Some(hive) due to [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.sql.catalogImplementation\".\n",
      "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110\n",
      "  warnings.warn(warn)\n",
      "/Users/mathiasgredal/git/Boston-Transport-Department/.venv/lib/python3.10/site-packages/pyspark/sql/connect/conf.py:64: UserWarning: Failed to set spark.sql.warehouse.dir to Some(/user/hive/warehouse) due to [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.sql.warehouse.dir\".\n",
      "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .remote(\"sc://localhost:15002\")\n",
    "        .appName(\"hive\")\n",
    "        .enableHiveSupport()\n",
    "        .config(\"hive.metastore.uris\", \"thrift://localhost:9083\")\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# Load the Spark Connect jar\n",
    "spark_connect_jar = f\"{sysconfig.get_paths()['purelib']}/pyspark/jars/spark-connect_2.13-4.0.1.jar\"\n",
    "spark.addArtifacts(spark_connect_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5a1b33c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ride_id: string, tripduration: double, starttime: string, matched_temp_c: double, diff_seconds_to_obs: bigint]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS weather_data (name VARCHAR(64), age INT, gpa DECIMAL(3, 2)) STORED AS PARQUET LOCATION 'hdfs://hdfs-cluster-namenode-default-0.hdfs-cluster-namenode-default.bigdata.svc.cluster.local/user/hive/warehouse/weater_data'\")\n",
    "#spark.sql(\"INSERT INTO students (name, age, gpa) VALUES ('Hello world123456', 20, 3.5)\")\n",
    "#spark.sql(\"REFRESH TABLE students\")\n",
    "spark.sql(\"\"\"WITH rides AS (\n",
    "  SELECT\n",
    "    concat_ws('|', starttime, start_station_id, end_station_id, tripduration) AS ride_id,\n",
    "    tripduration,\n",
    "    starttime\n",
    "  FROM bike_data\n",
    "),\n",
    "wx AS (\n",
    "  SELECT\n",
    "    observation_date,\n",
    "    dry_bulb_temperature_celsius\n",
    "  FROM weather_data\n",
    "),\n",
    "matched AS (\n",
    "  SELECT\n",
    "    r.ride_id,\n",
    "    r.tripduration,\n",
    "    r.starttime,\n",
    "    w.dry_bulb_temperature_celsius,\n",
    "    abs(\n",
    "      unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") -\n",
    "      unix_timestamp(w.observation_date, \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "    ) AS diff_seconds_to_obs,\n",
    "    row_number() OVER (\n",
    "      PARTITION BY r.ride_id\n",
    "      ORDER BY abs(\n",
    "        unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") -\n",
    "        unix_timestamp(w.observation_date, \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    "      )\n",
    "    ) AS rn\n",
    "  FROM rides r\n",
    "  JOIN wx w\n",
    "    ON w.observation_date BETWEEN\n",
    "         from_unixtime(unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") - 3600)\n",
    "     AND from_unixtime(unix_timestamp(r.starttime, \"yyyy-MM-dd'T'HH:mm:ss\") + 3600)\n",
    ")\n",
    "SELECT\n",
    "  ride_id,\n",
    "  tripduration,\n",
    "  starttime,\n",
    "  dry_bulb_temperature_celsius AS matched_temp_c,\n",
    "  diff_seconds_to_obs\n",
    "FROM matched\n",
    "WHERE rn = 1;\"\"\")\n",
    "#spark.sql(\"clear cache\")\n",
    "#spark.sql(\"INSERT OVERWRITE TABLE students SELECT * FROM students;\")\n",
    "# mydf = spark.sql(\"select * from weather_data\")\n",
    "# mydf.show()\n",
    "\n",
    "\n",
    "\n",
    "#CREATE TABLE students ( name STRING, age INT, gpa DECIMAL(3,2) ) STORED AS PARQUET TBLPROPERTIES ('transactional'='true') LOCATION 'hdfs://hdfs-cluster-namenode-default-0.hdfs-cluster-namenode-default.bigdata.svc.cluster.local/user/hive/warehouse/students2';\n",
    "#INSERT OVERWRITE TABLE students SELECT * FROM students;\n",
    "#spark.sql(\"ALTER TABLE students3 SET TBLPROPERTIES('transactional'='true')\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
