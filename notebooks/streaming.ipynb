{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c653e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stackable/spark/python/pyspark/sql/connect/conf.py:64: UserWarning: Failed to set spark.sql.catalogImplementation to Some(hive) due to [CANNOT_MODIFY_CONFIG] Cannot modify the value of the Spark config: \"spark.sql.catalogImplementation\".\n",
      "See also 'https://spark.apache.org/docs/latest/sql-migration-guide.html#ddl-statements'. SQLSTATE: 46110\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .remote(\"sc://spark-connect-server:15002\")\n",
    "        .appName(\"streaming-example\")\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-cluster-metastore:9083\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.addArtifacts(\"/stackable/spark/connect/spark-connect-4.0.1.jar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ec15a",
   "metadata": {},
   "source": [
    "Delete checkpoint:\n",
    "`kubectl -n bigdata exec -c namenode -it hdfs-cluster-namenode-default-0 -- /bin/bash -c \"./bin/hdfs dfs -rm -r /tmp/wordcount-checkpoint\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a45860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped previous query\n",
      "Nearest weather per ride streaming query started (Type safe version)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "try:\n",
    "    from pyspark.sql.avro.functions import from_avro\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Stop any existing streaming query\n",
    "try:\n",
    "    if 'query' in globals():\n",
    "        query.stop()\n",
    "        print(\"Stopped previous query\")\n",
    "except NameError:\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "# Configuration\n",
    "SCHEMA_REGISTRY_URL = \"http://schema-registry.bigdata.svc.cluster.local:8081\"\n",
    "KAFKA_BOOTSTRAP = \"kafka-broker.bigdata.svc.cluster.local:9092\"\n",
    "\n",
    "def get_latest_schema(subject):\n",
    "    \"\"\"Fetch latest schema string from Schema Registry\"\"\"\n",
    "    try:\n",
    "        url = f\"{SCHEMA_REGISTRY_URL}/subjects/{subject}/versions/latest\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"schema\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching schema for {subject}: {e}\")\n",
    "        raise\n",
    "\n",
    "# 1. Fetch Schemas\n",
    "try:\n",
    "    bike_schema_json = get_latest_schema(\"bike-data-value\")\n",
    "    weather_schema_json = get_latest_schema(\"weather-data-value\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to fetch schemas.\")\n",
    "    raise e\n",
    "\n",
    "# 2. Read Bike Data\n",
    "bikes = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", \"bike-data\") \n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    "    .select(\n",
    "        from_avro(\n",
    "            F.expr(\"substring(value, 6, length(value)-5)\"), \n",
    "            bike_schema_json\n",
    "        ).alias(\"bike\")\n",
    "    )\n",
    "    # Explicitly select and cast to ensure timestamp type (avoiding ambiguous column replacement)\n",
    "    .select(\n",
    "        F.col(\"bike.tripduration\"),\n",
    "        F.col(\"bike.start_station_id\"),\n",
    "        F.col(\"bike.end_station_id\"),\n",
    "        F.to_timestamp(F.col(\"bike.starttime\")).alias(\"starttime\")\n",
    "    )\n",
    "    .withColumn(\"join_key\", F.lit(1))\n",
    "    .alias(\"bikes\")\n",
    "    .withWatermark(\"starttime\", \"10 minutes\")\n",
    ")\n",
    "\n",
    "# 3. Read Weather Data\n",
    "weather = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", \"weather-data\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    "    .select(\n",
    "        from_avro(\n",
    "            F.expr(\"substring(value, 6, length(value)-5)\"), \n",
    "            weather_schema_json\n",
    "        ).alias(\"weather\")\n",
    "    )\n",
    "    # Explicitly select and cast\n",
    "    .select(\n",
    "        F.col(\"weather.dry_bulb_temperature_celsius\"),\n",
    "        F.to_timestamp(F.col(\"weather.observation_date\")).alias(\"observation_date\")\n",
    "    )\n",
    "    .withColumn(\"join_key\", F.lit(1))\n",
    "    .alias(\"weather\")\n",
    "    .withWatermark(\"observation_date\", \"10 minutes\")\n",
    ")\n",
    "\n",
    "# 4. Interval Join & Nearest Neighbor Logic\n",
    "joined = bikes.join(\n",
    "    weather,\n",
    "    (F.col(\"bikes.join_key\") == F.col(\"weather.join_key\")) &\n",
    "    (F.col(\"weather.observation_date\") >= F.col(\"bikes.starttime\") - F.expr(\"interval 1 hour\")) &\n",
    "    (F.col(\"weather.observation_date\") <= F.col(\"bikes.starttime\") + F.expr(\"interval 1 hour\")),\n",
    "    \"inner\"\n",
    ").withColumn(\n",
    "    \"time_diff\", \n",
    "    F.abs(F.col(\"bikes.starttime\").cast(\"long\") - F.col(\"weather.observation_date\").cast(\"long\"))\n",
    ")\n",
    "\n",
    "# Aggregate\n",
    "result = joined.groupBy(\n",
    "    \"bikes.tripduration\", \"bikes.starttime\", \"bikes.start_station_id\", \"bikes.end_station_id\"\n",
    ").agg(\n",
    "    F.min_by(\n",
    "        F.struct(\"weather.observation_date\", \"weather.dry_bulb_temperature_celsius\"), \n",
    "        F.col(\"time_diff\")\n",
    "    ).alias(\"nearest_weather\")\n",
    ").select(\n",
    "    F.col(\"starttime\"),\n",
    "    F.col(\"start_station_id\"),\n",
    "    F.col(\"end_station_id\"),\n",
    "    F.col(\"nearest_weather.dry_bulb_temperature_celsius\").alias(\"temp_c\"),\n",
    "    F.col(\"nearest_weather.observation_date\").alias(\"weather_ts\"),\n",
    "    F.col(\"tripduration\")\n",
    ")\n",
    "\n",
    "# 5. Write Output\n",
    "payload = result.select(\n",
    "    F.col(\"start_station_id\").alias(\"key\"),\n",
    "    F.to_json(F.struct(\"*\")).alias(\"value\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "    payload.writeStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"topic\", \"nearest-weather-output\")\n",
    "    .option(\"checkpointLocation\", \"/tmp/nearest-weather-checkpoint\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"1 second\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Nearest weather per ride streaming query started (Type safe version)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc8d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands to view word count results:\n",
    "#\n",
    "# 1. Create the output topic (if it doesn't exist):\n",
    "#    kubectl exec -n bigdata -it deployment/broker -- /opt/kafka/bin/kafka-topics.sh --create --topic wordcount-output --bootstrap-server broker:29092 --partitions 1 --replication-factor 1\n",
    "#\n",
    "# 2. View word counts from the output topic (from beginning):\n",
    "#    kubectl exec -n bigdata -it deployment/broker -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server broker:29092 --topic wordcount-output --from-beginning\n",
    "#\n",
    "# 3. View only new messages (real-time):\n",
    "#    kubectl exec -n bigdata -it deployment/broker -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server broker:29092 --topic wordcount-output\n",
    "#\n",
    "# 4. List all topics:\n",
    "#    kubectl exec -n bigdata -it deployment/broker -- /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server broker:29092\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc71c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the query running\n",
    "# Press Ctrl+C or interrupt the kernel to stop\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Send test messages with text for word counting\n",
    "import kafka\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Create producer\n",
    "producer = kafka.KafkaProducer(\n",
    "    bootstrap_servers=['kafka-broker.bigdata.svc.cluster.local:9092'],\n",
    "    value_serializer=lambda v: v.encode('utf-8') if isinstance(v, str) else v\n",
    ")\n",
    "\n",
    "# Sample sentences for word count testing\n",
    "test_messages = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Spark streaming is awesome for real-time processing\",\n",
    "    \"Kafka and Spark work great together\",\n",
    "    \"Word count is a classic example of stream processing\",\n",
    "    \"The fox jumps and the dog runs\",\n",
    "    \"Real-time analytics with Spark and Kafka\",\n",
    "    \"Streaming data processing made easy\",\n",
    "    \"Count words in real-time with Spark Streaming\"\n",
    "]\n",
    "\n",
    "print(\"Sending test messages for word counting...\")\n",
    "for i, message in enumerate(test_messages):\n",
    "    producer.send('sparktest', value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.5)  # Wait 0.5 seconds between messages (faster for demo)\n",
    "\n",
    "producer.flush()\n",
    "print(\"\\nAll test messages sent! Check the streaming output above for word counts.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
