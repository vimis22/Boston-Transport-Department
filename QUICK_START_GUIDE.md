# ðŸš€ BOSTON TRANSPORT - HURTIG OPSTARTSGUIDE

## ðŸ“Œ VIGTIGT AT FORSTÃ… FÃ˜RST

**Dit projekt starter IKKE fra `src/`-mappen!**

- **`infra/`** â†’ Her starter du projektet (Terraform deployer alt)
- **`src/`** â†’ Indeholder kildekode til applikationer (deployes automatisk af Terraform)
- **`tools/`** â†’ HjÃ¦lpescripts til port-forwarding og datahÃ¥ndtering
- **`mock-data/`** â†’ CSV-filer som Streamer lÃ¦ser

**Terraform lÃ¦ser modulerne i `infra/modules/` og deployer alt som Docker containers til Kubernetes.**

---

## ðŸ“‹ FORUDSÃ†TNINGER

- âœ… Docker Desktop installeret
- âœ… Python 3.11+
- âš ï¸ Kubernetes aktiveret i Docker Desktop (vigtigt!)
- âš ï¸ Terraform (skal installeres)
- âš ï¸ kubectl (installeres automatisk med Docker Desktop)

---

## TRIN 0: Start Kubernetes i Docker Desktop âš ï¸ VIGTIGT!

**FÃ¸r du kan kÃ¸re projektet, skal Kubernetes vÃ¦re aktiveret i Docker Desktop:**

1. **Ã…bn Docker Desktop**
2. **Klik pÃ¥ tandhjul-ikonet** (âš™ï¸ Settings) Ã¸verst til hÃ¸jre
3. **Klik pÃ¥ "Kubernetes"** i venstre menu
4. **SÃ¦t flueben ved "Enable Kubernetes"**
5. **Klik "Apply & restart"**
6. **Vent 2-3 minutter** mens Kubernetes starter (du ser en grÃ¸n indikator nederst til venstre)

### Verificer at Kubernetes kÃ¸rer:
```bash
kubectl cluster-info
```

Du skulle se noget som:
```
Kubernetes control plane is running at https://kubernetes.docker.internal:6443
CoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

**Hvis du fÃ¥r fejl "connection refused", er Kubernetes ikke startet endnu - vent lidt lÃ¦ngere.**

---

## TRIN 1: InstallÃ©r Terraform (2 minutter)

### Windows - PowerShell (KÃ¸r som Administrator):
```powershell
Set-ExecutionPolicy Bypass -Scope Process -Force
[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072
iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))

# Efter Chocolatey er installeret:
choco install terraform -y
```

Verificer:
```bash
terraform --version
```

---

## TRIN 2: Deploy Infrastrukturen (5-10 minutter)

**Dette er dit startpunkt!**

```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department\infra\environments\local
terraform init
terraform apply -auto-approve
```

### Hvad deployer Terraform?

Terraform lÃ¦ser moduler fra `infra/modules/` og deployer fÃ¸lgende services som Kubernetes pods:

1. **Kafka** + Schema Registry + REST Proxy (fra `infra/modules/kafka/`)
2. **HDFS** (Hadoop) - NameNode + DataNode (fra `infra/modules/hadoop/`)
3. **Spark** Connect Server + Workers (fra `infra/modules/bigdata/`)
4. **Hive** Metastore + Thrift Server (fra `infra/modules/hadoop/`)
5. **JupyterLab** (til notebooks)
6. **Time Manager** (simuleret tid til streaming, kode fra `src/timemanager/`)
7. **Streamer** (lÃ¦ser CSV â†’ sender til Kafka som Avro, kode fra `src/streamer/`)
8. **Data Analysis ETL** (dit Spark job, kode fra `src/etl/`)
9. **Dashboard** (visualiserer resultater, kode fra `src/dashboard/`)

**Alt kÃ¸rer i Kubernetes namespace `bigdata`.**

---

## TRIN 3: Vent pÃ¥ at Pods Starter (2-3 minutter)

```bash
kubectl get pods -n bigdata -w
```

Vent til alle pods viser `Running` status. Tryk `Ctrl+C` nÃ¥r fÃ¦rdig.

---

## TRIN 4: Port-Forward til Services

**Ã…bn en NY terminal** og kÃ¸r:

```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department
python tools/forward-all.py
```

Dette port-forwarder:
- **Dashboard**: http://localhost:3000
- **HDFS UI**: http://localhost:9870
- **Spark UI**: http://localhost:4040
- **JupyterLab**: http://localhost:8080 (token: `adminadmin`)
- **Kafka REST Proxy**: http://localhost:8083
- **Schema Registry**: http://localhost:8081
- **Time Manager API**: http://localhost:8000
- **Hive**: localhost:10000

---

## TRIN 5: Start Data Pipeline

### 5.1 Tjek Time Manager Status
```bash
curl http://localhost:8000/api/v1/clock
```

### 5.2 Start Simulering (Streamer Data)
```bash
curl -X POST http://localhost:8000/api/v1/clock/start
```

### 5.3 OvervÃ¥g Logs

**Streamer (producerer til Kafka):**
```bash
kubectl logs -n bigdata -l app=streamer -f
```

**Data Analysis ETL (dine beregninger):**
```bash
kubectl logs -n bigdata -l app=data-analysis -f
```

---

## ðŸ”„ FORSTÃ… DATAFLOWET

```
1. CSV-filer (mock-data/)
   â†“
2. Time Manager (src/timemanager/) â†’ Simulerer tid
   â†“
3. Streamer Pod (src/streamer/) â†’ LÃ¦ser CSV â†’ Konverterer til Avro â†’ Kafka
   â†“
4. Kafka Topics (bike-data, taxi-data, weather-data, accident-data)
   â†“
5. Data Analysis ETL Pod (src/etl/)
   â”œâ”€â”€ Forbinder til Spark Connect Server
   â”œâ”€â”€ Spark lÃ¦ser fra Kafka
   â”œâ”€â”€ Spark udfÃ¸rer beregninger (vejr-transport korrelationer)
   â””â”€â”€ Spark skriver Parquet-filer:
       â”œâ”€â”€ /data/processed_simple/  (transformeret rÃ¥data)
       â””â”€â”€ /data/analytics/         (beregnede korrelationer)
   â†“
6. Hive Metastore â†’ Indekserer Parquet-filerne
   â†“
7. Dashboard (src/dashboard/) â†’ Henter data via Hive HTTP Proxy â†’ Viser grafer
```

---

## âœ… VERIFICER DATAFLOW

### 6.1 Tjek Kafka Topics
```bash
kubectl exec -n bigdata svc/kafka-broker -- kafka-topics --bootstrap-server localhost:9092 --list
```

Skulle vise: `bike-data`, `taxi-data`, `weather-data`, `accident-data`

### 6.2 Tjek Schemas i Registry
```bash
curl http://localhost:8081/subjects
```

Skulle vise: `["bike-data-value", "taxi-data-value", "weather-data-value", "accident-data-value"]`

### 6.3 Tjek ETL Output i Spark Pod
```bash
# Vis output-mapper
kubectl exec -n bigdata deployment/data-analysis -- ls -la /data/processed_simple/
kubectl exec -n bigdata deployment/data-analysis -- ls -la /data/analytics/
```

Skulle vise mapper som:
- `/data/processed_simple/bike_trips/`
- `/data/processed_simple/taxi_trips/`
- `/data/processed_simple/weather_data/`
- `/data/analytics/weather_transport_correlation/`
- `/data/analytics/pearson_correlations/`

### 6.4 Tjek Parquet-filer
```bash
kubectl exec -n bigdata deployment/data-analysis -- find /data/processed_simple -name "*.parquet" | head -10
kubectl exec -n bigdata deployment/data-analysis -- find /data/analytics -name "*.parquet" | head -10
```

---

## ðŸ“Š VIS RESULTATER I DASHBOARD

### Option A: Dashboard via Kubernetes (Automatisk)
Dashboard er allerede deployed via Terraform og kÃ¸rer som en pod.

1. **Port-forward dashboard:**
   ```bash
   kubectl port-forward -n bigdata svc/dashboard 3000:3000
   ```

2. **Ã…bn Dashboard:**
   - GÃ¥ til: http://localhost:3000

### Option B: Dashboard Lokal Udvikling (Med Bun)

Hvis du vil kÃ¸re dashboard lokalt for udvikling:

1. **Installer Bun (hvis ikke installeret):**
   ```bash
   curl -fsSL https://bun.sh/install | bash
   ```

2. **Installer dependencies:**
   ```bash
   cd src/dashboard
   bun install
   ```

3. **Port-forward nÃ¸dvendige services:**
   ```bash
   # I separate terminaler eller brug tmux/screen:
   kubectl port-forward -n bigdata svc/timemanager 8000:8000
   kubectl port-forward -n bigdata svc/hive-http-proxy 10001:10001
   kubectl port-forward -n bigdata svc/kafka-ui 8083:8080
   ```

4. **Start dashboard lokalt:**
   ```bash
   cd src/dashboard
   bun run src/index.ts
   ```

5. **Ã…bn Dashboard:**
   - GÃ¥ til: http://localhost:3000

### Hvad Du Skulle Se:
- Live transportdata (cykel/taxi-ture)
- Vejrkorrelationer
- Realtidsanalyse-grafer
- Kafka topics overview

---

## ðŸ§© VIGTIGE SPÃ˜RGSMÃ…L & SVAR

### Q1: "Hvor skrives mit ETL output?"

**Dit ETL output skrives til:**
- `/data/processed_simple/` - Transformeret rÃ¥data som Parquet
- `/data/analytics/` - Beregnede korrelationer som Parquet

Disse er **Persistent Volumes** i Kubernetes som:
1. Spark skriver direkte til
2. Hive lÃ¦ser fra (via Hive Metastore)
3. Dashboard forespÃ¸rger via Hive

**IKKE direkte til HDFS** - men Hive kan konfigureres til at bruge HDFS som backend (dit setup bruger PVC).

### Q2: "Forwarded min ETL-kode til Spark eller Hadoop?"

**Din ETL-kode ER en Spark-applikation.** Den:
- Forbinder til Spark Connect Server (`sc://spark-connect-server:15002`)
- Indsender Spark Structured Streaming jobs
- Spark executors udfÃ¸rer beregningerne
- Spark executors skriver resultaterne til `/data/...`

**Den "forwarder" IKKE til Spark - den KÃ˜RER PÃ… Spark!**

### Q3: "Hvad er forskellen pÃ¥ `infra/` og `src/`?"

| Folder | Rolle |
|--------|-------|
| **`infra/`** | Terraform moduler - **DIT OPSTARTSPUNKT** |
| **`src/`** | Kildekode til applikationer (deployes af Terraform) |
| **`tools/`** | HjÃ¦lpescripts (port-forward, upload data osv.) |
| **`mock-data/`** | CSV-filer som Streamer lÃ¦ser |
| **`notebooks/`** | Jupyter notebooks til dataanalyse |

---

## ðŸ”§ FEJLFINDING

### Kubernetes Connection Refused
**Fejl:** `dial tcp 127.0.0.1:6443: connectex: No connection could be made because the target machine actively refused it.`

**Ã…rsag:** Kubernetes er ikke startet i Docker Desktop.

**Fix:**
1. Ã…bn Docker Desktop
2. GÃ¥ til Settings â†’ Kubernetes
3. AktivÃ©r "Enable Kubernetes"
4. Klik "Apply & restart"
5. Vent 2-3 minutter
6. Verificer: `kubectl cluster-info`

### ETL Pod Crasher med "404 schema not found"
**Fix:**
```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department
python tools/create-schemas.py
```

### Streamer Crasher - Manglende Parquet Filer i HDFS
**Fejl:** Streamer kan ikke finde `/bigdata/*.parquet` filer

**Fix - Upload datasÃ¦t til HDFS:**
```bash
# 1. Download og konverter datasÃ¦t (krÃ¦ver gdown og duckdb)
pip install gdown duckdb
python tools/create-datasets.py

# 2. Upload til HDFS via kubectl (efter datasets er genereret)
cat boston_datasets/bigdata/weather_data.parquet | kubectl exec -i -n bigdata hdfs-cluster-namenode-default-0 -c namenode -- sh -c "cat > /tmp/weather_data.parquet && hdfs dfs -put -f /tmp/weather_data.parquet /bigdata/weather_data.parquet && rm /tmp/weather_data.parquet"

cat boston_datasets/bigdata/taxi_data.parquet | kubectl exec -i -n bigdata hdfs-cluster-namenode-default-0 -c namenode -- sh -c "cat > /tmp/taxi_data.parquet && hdfs dfs -put -f /tmp/taxi_data.parquet /bigdata/taxi_data.parquet && rm /tmp/taxi_data.parquet"

cat boston_datasets/bigdata/bike_data.parquet | kubectl exec -i -n bigdata hdfs-cluster-namenode-default-0 -c namenode -- sh -c "cat > /tmp/bike_data.parquet && hdfs dfs -put -f /tmp/bike_data.parquet /bigdata/bike_data.parquet && rm /tmp/bike_data.parquet"

# 3. Verificer upload
kubectl exec -n bigdata hdfs-cluster-namenode-default-0 -c namenode -- sh -c "hdfs dfs -ls -h /bigdata"
```

### Ingen Data Vises i Dashboard
**Tjek:**
1. KÃ¸rer time manager? `curl http://localhost:8000/api/v1/clock`
2. Producerer streamer? `kubectl logs -n bigdata -l app=streamer`
3. KÃ¸rer ETL? `kubectl logs -n bigdata -l app=data-analysis`
4. Er Parquet-filer oprettet? `kubectl exec -n bigdata deployment/data-analysis -- ls /data/analytics/`

### Pods Starter Ikke
```bash
# Tjek pod status
kubectl get pods -n bigdata

# Tjek specifikke pod logs
kubectl logs -n bigdata <pod-name>

# Beskriv pod for events
kubectl describe pod -n bigdata <pod-name>
```

---

## ðŸ§¹ RYDNING

For at slette alt:
```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department\infra\environments\local
terraform destroy -auto-approve
```

Eller manuelt:
```bash
kubectl delete namespace bigdata
```

---

## ðŸš€ NÃ†STE SKRIDT

1. Udforsk JupyterLab: http://localhost:8080 (token: `adminadmin`)
2. ForespÃ¸rg Hive direkte:
   ```bash
   kubectl exec -n bigdata svc/spark-thrift-service -- beeline -u jdbc:hive2://localhost:10000
   ```
3. Se Spark UI: http://localhost:4040
4. Modificer ETL-kode i `src/etl/jobs/data_analysis.py` og redeploy

---

**Held og lykke! ðŸŽ¯**
