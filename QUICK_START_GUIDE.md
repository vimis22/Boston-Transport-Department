# üöÄ BOSTON TRANSPORT - HURTIG OPSTARTSGUIDE

## üìå VIGTIGT AT FORST√Ö F√òRST

**Dit projekt starter IKKE fra `src/`-mappen!**

- **`infra/`** ‚Üí Her starter du projektet (Terraform deployer alt)
- **`src/`** ‚Üí Indeholder kildekode til applikationer (deployes automatisk af Terraform)
- **`tools/`** ‚Üí Hj√¶lpescripts til port-forwarding og datah√•ndtering
- **`mock-data/`** ‚Üí CSV-filer som Streamer l√¶ser

**Terraform l√¶ser modulerne i `infra/modules/` og deployer alt som Docker containers til Kubernetes.**

---

## üìã FORUDS√ÜTNINGER

- ‚úÖ Docker Desktop installeret
- ‚úÖ Python 3.11+
- ‚ö†Ô∏è Kubernetes aktiveret i Docker Desktop (vigtigt!)
- ‚ö†Ô∏è Terraform (skal installeres)
- ‚ö†Ô∏è kubectl (installeres automatisk med Docker Desktop)

---

## TRIN 0: Start Kubernetes i Docker Desktop ‚ö†Ô∏è VIGTIGT!

**F√∏r du kan k√∏re projektet, skal Kubernetes v√¶re aktiveret i Docker Desktop:**

1. **√Öbn Docker Desktop**
2. **Klik p√• tandhjul-ikonet** (‚öôÔ∏è Settings) √∏verst til h√∏jre
3. **Klik p√• "Kubernetes"** i venstre menu
4. **S√¶t flueben ved "Enable Kubernetes"**
5. **Klik "Apply & restart"**
6. **Vent 2-3 minutter** mens Kubernetes starter (du ser en gr√∏n indikator nederst til venstre)

### Verificer at Kubernetes k√∏rer:
```bash
kubectl cluster-info
```

Du skulle se noget som:
```
Kubernetes control plane is running at https://kubernetes.docker.internal:6443
CoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

**Hvis du f√•r fejl "connection refused", er Kubernetes ikke startet endnu - vent lidt l√¶ngere.**

---

## TRIN 1: Install√©r Terraform (2 minutter)

### Windows - PowerShell (K√∏r som Administrator):
```powershell
Set-ExecutionPolicy Bypass -Scope Process -Force
[System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072
iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))

# Efter Chocolatey er installeret:
choco install terraform -y
```

Verificer:
```bash
terraform --version
```

---

## TRIN 2: Deploy Infrastrukturen (5-10 minutter)

**Dette er dit startpunkt!**

```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department\infra\environments\local
terraform init
terraform apply -auto-approve
```

### Hvad deployer Terraform?

Terraform l√¶ser moduler fra `infra/modules/` og deployer f√∏lgende services som Kubernetes pods:

1. **Kafka** + Schema Registry + REST Proxy (fra `infra/modules/kafka/`)
2. **HDFS** (Hadoop) - NameNode + DataNode (fra `infra/modules/hadoop/`)
3. **Spark** Connect Server + Workers (fra `infra/modules/bigdata/`)
4. **Hive** Metastore + Thrift Server (fra `infra/modules/hadoop/`)
5. **JupyterLab** (til notebooks)
6. **Time Manager** (simuleret tid til streaming, kode fra `src/timemanager/`)
7. **Streamer** (l√¶ser CSV ‚Üí sender til Kafka som Avro, kode fra `src/streamer/`)
8. **Data Analysis ETL** (dit Spark job, kode fra `src/etl/`)
9. **Dashboard** (visualiserer resultater, kode fra `src/dashboard/`)

**Alt k√∏rer i Kubernetes namespace `bigdata`.**

---

## TRIN 3: Vent p√• at Pods Starter (2-3 minutter)

```bash
kubectl get pods -n bigdata -w
```

Vent til alle pods viser `Running` status. Tryk `Ctrl+C` n√•r f√¶rdig.

---

## TRIN 4: Port-Forward til Services

**√Öbn en NY terminal** og k√∏r:

```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department
python tools/forward-all.py
```

Dette port-forwarder:
- **Dashboard**: http://localhost:3000
- **HDFS UI**: http://localhost:9870
- **Spark UI**: http://localhost:4040
- **JupyterLab**: http://localhost:8080 (token: `adminadmin`)
- **Kafka REST Proxy**: http://localhost:8083
- **Schema Registry**: http://localhost:8081
- **Time Manager API**: http://localhost:8000
- **Hive**: localhost:10000

---

## TRIN 5: Start Data Pipeline

### 5.1 Tjek Time Manager Status
```bash
curl http://localhost:8000/api/v1/clock
```

### 5.2 Start Simulering (Streamer Data)
```bash
curl -X POST http://localhost:8000/api/v1/clock/start
```

### 5.3 Overv√•g Logs

**Streamer (producerer til Kafka):**
```bash
kubectl logs -n bigdata -l app=streamer -f
```

**Data Analysis ETL (dine beregninger):**
```bash
kubectl logs -n bigdata -l app=data-analysis -f
```

---

## üîÑ FORST√Ö DATAFLOWET

```
1. CSV-filer (mock-data/)
   ‚Üì
2. Time Manager (src/timemanager/) ‚Üí Simulerer tid
   ‚Üì
3. Streamer Pod (src/streamer/) ‚Üí L√¶ser CSV ‚Üí Konverterer til Avro ‚Üí Kafka
   ‚Üì
4. Kafka Topics (bike-data, taxi-data, weather-data, accident-data)
   ‚Üì
5. Data Analysis ETL Pod (src/etl/)
   ‚îú‚îÄ‚îÄ Forbinder til Spark Connect Server
   ‚îú‚îÄ‚îÄ Spark l√¶ser fra Kafka
   ‚îú‚îÄ‚îÄ Spark udf√∏rer beregninger (vejr-transport korrelationer)
   ‚îî‚îÄ‚îÄ Spark skriver Parquet-filer:
       ‚îú‚îÄ‚îÄ /data/processed_simple/  (transformeret r√•data)
       ‚îî‚îÄ‚îÄ /data/analytics/         (beregnede korrelationer)
   ‚Üì
6. Hive Metastore ‚Üí Indekserer Parquet-filerne
   ‚Üì
7. Dashboard (src/dashboard/) ‚Üí Henter data via Hive HTTP Proxy ‚Üí Viser grafer
```

---

## ‚úÖ VERIFICER DATAFLOW

### 6.1 Tjek Kafka Topics
```bash
kubectl exec -n bigdata svc/kafka-broker -- kafka-topics --bootstrap-server localhost:9092 --list
```

Skulle vise: `bike-data`, `taxi-data`, `weather-data`, `accident-data`

### 6.2 Tjek Schemas i Registry
```bash
curl http://localhost:8081/subjects
```

Skulle vise: `["bike-data-value", "taxi-data-value", "weather-data-value", "accident-data-value"]`

### 6.3 Tjek ETL Output i Spark Pod
```bash
# Vis output-mapper
kubectl exec -n bigdata deployment/data-analysis -- ls -la /data/processed_simple/
kubectl exec -n bigdata deployment/data-analysis -- ls -la /data/analytics/
```

Skulle vise mapper som:
- `/data/processed_simple/bike_trips/`
- `/data/processed_simple/taxi_trips/`
- `/data/processed_simple/weather_data/`
- `/data/analytics/weather_transport_correlation/`
- `/data/analytics/pearson_correlations/`

### 6.4 Tjek Parquet-filer
```bash
kubectl exec -n bigdata deployment/data-analysis -- find /data/processed_simple -name "*.parquet" | head -10
kubectl exec -n bigdata deployment/data-analysis -- find /data/analytics -name "*.parquet" | head -10
```

---

## üìä VIS RESULTATER I DASHBOARD

1. **√Öbn Dashboard:**
   - G√• til: http://localhost:3000

2. **Du skulle se:**
   - Live transportdata (cykel/taxi-ture)
   - Vejrkorrelationer
   - Realtidsanalyse-grafer

---

## üß© VIGTIGE SP√òRGSM√ÖL & SVAR

### Q1: "Hvor skrives mit ETL output?"

**Dit ETL output skrives til:**
- `/data/processed_simple/` - Transformeret r√•data som Parquet
- `/data/analytics/` - Beregnede korrelationer som Parquet

Disse er **Persistent Volumes** i Kubernetes som:
1. Spark skriver direkte til
2. Hive l√¶ser fra (via Hive Metastore)
3. Dashboard foresp√∏rger via Hive

**IKKE direkte til HDFS** - men Hive kan konfigureres til at bruge HDFS som backend (dit setup bruger PVC).

### Q2: "Forwarded min ETL-kode til Spark eller Hadoop?"

**Din ETL-kode ER en Spark-applikation.** Den:
- Forbinder til Spark Connect Server (`sc://spark-connect-server:15002`)
- Indsender Spark Structured Streaming jobs
- Spark executors udf√∏rer beregningerne
- Spark executors skriver resultaterne til `/data/...`

**Den "forwarder" IKKE til Spark - den K√òRER P√Ö Spark!**

### Q3: "Hvad er forskellen p√• `infra/` og `src/`?"

| Folder | Rolle |
|--------|-------|
| **`infra/`** | Terraform moduler - **DIT OPSTARTSPUNKT** |
| **`src/`** | Kildekode til applikationer (deployes af Terraform) |
| **`tools/`** | Hj√¶lpescripts (port-forward, upload data osv.) |
| **`mock-data/`** | CSV-filer som Streamer l√¶ser |
| **`notebooks/`** | Jupyter notebooks til dataanalyse |

---

## üîß FEJLFINDING

### Kubernetes Connection Refused
**Fejl:** `dial tcp 127.0.0.1:6443: connectex: No connection could be made because the target machine actively refused it.`

**√Örsag:** Kubernetes er ikke startet i Docker Desktop.

**Fix:**
1. √Öbn Docker Desktop
2. G√• til Settings ‚Üí Kubernetes
3. Aktiv√©r "Enable Kubernetes"
4. Klik "Apply & restart"
5. Vent 2-3 minutter
6. Verificer: `kubectl cluster-info`

### ETL Pod Crasher med "404 schema not found"
**Fix:**
```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department
python tools/create-schemas.py
```

### Ingen Data Vises i Dashboard
**Tjek:**
1. K√∏rer time manager? `curl http://localhost:8000/api/v1/clock`
2. Producerer streamer? `kubectl logs -n bigdata -l app=streamer`
3. K√∏rer ETL? `kubectl logs -n bigdata -l app=data-analysis`
4. Er Parquet-filer oprettet? `kubectl exec -n bigdata deployment/data-analysis -- ls /data/analytics/`

### Pods Starter Ikke
```bash
# Tjek pod status
kubectl get pods -n bigdata

# Tjek specifikke pod logs
kubectl logs -n bigdata <pod-name>

# Beskriv pod for events
kubectl describe pod -n bigdata <pod-name>
```

---

## üßπ RYDNING

For at slette alt:
```bash
cd C:\Users\vivek\Downloads\Boston-Transport-Department\infra\environments\local
terraform destroy -auto-approve
```

Eller manuelt:
```bash
kubectl delete namespace bigdata
```

---

## üöÄ N√ÜSTE SKRIDT

1. Udforsk JupyterLab: http://localhost:8080 (token: `adminadmin`)
2. Foresp√∏rg Hive direkte:
   ```bash
   kubectl exec -n bigdata svc/spark-thrift-service -- beeline -u jdbc:hive2://localhost:10000
   ```
3. Se Spark UI: http://localhost:4040
4. Modificer ETL-kode i `src/etl/jobs/data_analysis.py` og redeploy

---

**Held og lykke! üéØ**
